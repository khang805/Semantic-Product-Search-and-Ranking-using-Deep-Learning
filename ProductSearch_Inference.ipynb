{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8046110,"sourceType":"datasetVersion","datasetId":4744475},{"sourceId":13817179,"sourceType":"datasetVersion","datasetId":8798820}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# üü¢ CELL 1: SETUP\n!pip install -q sentence-transformers gradio\nimport os\nimport shutil\nimport zipfile\nimport pandas as pd\nfrom sentence_transformers import CrossEncoder\nimport gradio as gr\n\nprint(\"‚úÖ Libraries Installed\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:17:18.914205Z","iopub.execute_input":"2025-11-21T13:17:18.914775Z","iopub.status.idle":"2025-11-21T13:17:22.762833Z","shell.execute_reply.started":"2025-11-21T13:17:18.914754Z","shell.execute_reply":"2025-11-21T13:17:22.761777Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Libraries Installed\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# üü¢ CELL 2: MASTER LOAD (Model + Clean Data)\nimport os\nimport pandas as pd\nfrom sentence_transformers import CrossEncoder\n\n# --- 1. AUTO-DETECT PATHS ---\n# Kaggle input paths can vary, so we scan for the files we need.\nmodel_path = None\nproducts_file_path = None\n\nprint(\"üîç Scanning Input Directory...\")\nfor root, dirs, files in os.walk('/kaggle/input'):\n    # Find Model: Look for config.json inside a folder\n    if \"config.json\" in files and (\"pytorch_model.bin\" in files or \"model.safetensors\" in files):\n        model_path = root\n        print(f\"   ‚úÖ FOUND MODEL at: {model_path}\")\n    \n    # Find Dataset: Look for the specific parquet file\n    if \"shopping_queries_dataset_products.parquet\" in files:\n        products_file_path = os.path.join(root, \"shopping_queries_dataset_products.parquet\")\n        print(f\"   ‚úÖ FOUND DATASET at: {products_file_path}\")\n\n# --- 2. LOAD MODEL ---\ntry:\n    if model_path:\n        print(f\"üß† Loading Model...\")\n        model = CrossEncoder(model_path)\n        print(\"‚úÖ Model Loaded successfully.\")\n    else:\n        print(\"‚ùå ERROR: Model not found. Did you add your previous notebook as output?\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading model: {e}\")\n\n# --- 3. LOAD & CLEAN DATA ---\nprint(\"üìö Loading Product Catalog...\")\ntry:\n    if products_file_path:\n        df_products = pd.read_parquet(products_file_path)\n        \n        # Filter for US English\n        df_products = df_products[df_products['product_locale'] == 'us']\n        \n        # --- CRITICAL FIX: CLEAN GARBAGE ROWS ---\n        # 1. Remove rows with empty titles\n        df_products = df_products.dropna(subset=['product_title'])\n        # 2. Remove junk titles like \"1.0\", \"2.0\" (Must be longer than 5 chars)\n        df_products = df_products[df_products['product_title'].str.len() > 5]\n        \n        # Sample 20,000 products for fast demo search\n        df_products = df_products.sample(n=20000, random_state=42).fillna('')\n        \n        # Create search text field\n        df_products['text'] = df_products['product_title'] + \" \" + df_products['product_description']\n        \n        print(f\"‚úÖ Catalog Ready: {len(df_products)} clean products loaded.\")\n    else:\n        print(\"‚ùå ERROR: Dataset not found. Did you add 'amazon-esci' to Inputs?\")\n\nexcept Exception as e:\n    print(f\"‚ùå Error loading dataset: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:24:55.811597Z","iopub.execute_input":"2025-11-21T13:24:55.811875Z","iopub.status.idle":"2025-11-21T13:25:09.694877Z","shell.execute_reply.started":"2025-11-21T13:24:55.811854Z","shell.execute_reply":"2025-11-21T13:25:09.694083Z"}},"outputs":[{"name":"stdout","text":"üîç Scanning Input Directory...\n   ‚úÖ FOUND DATASET at: /kaggle/input/amazon-esci/shopping_queries_dataset/shopping_queries_dataset_products.parquet\n   ‚úÖ FOUND MODEL at: /kaggle/input/dataset\nüß† Loading Model...\n‚úÖ Model Loaded successfully.\nüìö Loading Product Catalog...\n‚úÖ Catalog Ready: 20000 clean products loaded.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# üü¢ CELL 4: FINAL DEMO UI (Robust)\ndef search_products(user_query):\n    # 1. RETRIEVAL PHASE (Simulation)\n    # We grab 100 candidates.\n    # Priority 1: Title contains query words (High likelihood matches)\n    # Priority 2: Random fill (Distractors to test the AI's ability to filter junk)\n    \n    # Split query into words to be more flexible\n    query_words = user_query.lower().split()\n    primary_keyword = query_words[0] if query_words else \"\"\n    \n    # Find items containing at least the first word\n    candidates = df_products[df_products['text'].str.contains(primary_keyword, case=False, regex=False)].head(50)\n    \n    # If we have fewer than 50, fill with random items\n    if len(candidates) < 50:\n        remaining = 50 - len(candidates)\n        fillers = df_products.sample(n=remaining)\n        candidates = pd.concat([candidates, fillers])\n        \n    candidate_texts = candidates['text'].tolist()\n    candidate_titles = candidates['product_title'].tolist()\n    \n    # 2. AI RE-RANKING PHASE (The \"Intelligence\")\n    # The Cross-Encoder looks at the full pair (Query, Product)\n    pairs = [[user_query, prod] for prod in candidate_texts]\n    scores = model.predict(pairs)\n    \n    # 3. SORTING\n    results = list(zip(candidate_titles, scores))\n    results.sort(key=lambda x: x[1], reverse=True)\n    \n    # 4. FORMATTING\n    out = f\"üîç Query: '{user_query}'\\n\"\n    out += f\"‚ö° AI Re-ranked {len(candidates)} candidates in real-time.\\n\"\n    out += \"=\"*50 + \"\\n\\n\"\n    \n    for i, (title, score) in enumerate(results[:10]): # Show Top 10\n        # Visual Indicator for Relevance\n        if score > 0.8:\n            icon = \"üü¢ Excellent Match\"\n        elif score > 0.4:\n            icon = \"üü° Potential Match\"\n        else:\n            icon = \"üî¥ Low Relevance\"\n            \n        out += f\"{i+1}. {title}\\n\"\n        out += f\"   [{score:.4f}] {icon}\\n\"\n        out += \"-\"*30 + \"\\n\"\n        \n    return out\n\n# Launch\niface = gr.Interface(\n    fn=search_products,\n    inputs=gr.Textbox(label=\"Search Amazon\", placeholder=\"Try: 'running shoes', 'wireless charger', 'ps5 games'\"),\n    outputs=gr.Textbox(label=\"AI Ranked Results\", lines=20),\n    title=\"üõçÔ∏è Intelligent Product Search (Task 2 Demo)\",\n    description=\"This search engine uses a BERT Cross-Encoder to semantically understand your query and re-rank products.\",\n    examples=[\n        [\"wireless gaming mouse\"],\n        [\"running shoes for men\"],\n        [\"iphone 12 pro max case\"],\n        [\"yoga mat non slip\"],\n        [\"coffee maker with timer\"]\n    ]\n)\n\niface.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:25:14.907511Z","iopub.execute_input":"2025-11-21T13:25:14.908047Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://2ed699c261642866a1.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://2ed699c261642866a1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":null}]}